{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level language modeling in PyTorch\n",
    "\n",
    "In the model that we will build now, the input is a text document, and our goal is to develop a model that can generate\n",
    "new text that is similar in style to the input document.\n",
    "\n",
    "In character-level language modeling, the input is broken down into a sequence of characters that are fed into our\n",
    "network one character at a time. The network will process each new character in conjunction with the memory of the\n",
    "previously seen characters to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1144k  100 1144k    0     0  1347k      0 --:--:-- --:--:-- --:--:-- 1346k\n"
     ]
    }
   ],
   "source": [
    "# Downloading the dataset\n",
    "!curl -O https://raw.githubusercontent.com/rasbt/machine-learning-book/refs/heads/main/ch15/1268-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 1112350\n",
      "Unique charcters: 80\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the dataset\n",
    "import numpy as np\n",
    "with open('1268-0.txt','r',encoding='utf-8') as fp:\n",
    "    text = fp.read()\n",
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "text = text[start_idx:end_indx]\n",
    "char_set = set(text)\n",
    "print(f\"Total length: {len(text)}\")\n",
    "print(f\"Unique charcters: {len(char_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need a way to convert characters into integer values and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE MYSTERIOUS  ===> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28] ===> ISLAND\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array([char2int[ch] for ch in text],dtype=np.int32)\n",
    "print(text[:15],\"===>\",text_encoded[:15])\n",
    "print(text_encoded[15:21],\"===>\",\"\".join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to design a model that can predict the next character of a given input sequence, where the input\n",
    "sequence represents an incomplete text. This problem can be thinked of as a multiclass classification task.\n",
    "\n",
    "Let's firstly clip the sequence length to 40. In practice, the sequence length impacts the quality of the generated\n",
    "text. Longer sequences can result in more meaningful sentences. For shorter sequences, however, the model might focus\n",
    "on capturing individual words correctly, while ignoring the context for the most part.\n",
    "\n",
    "Thus, in practice, finding a sweet spot and good value for the sequence length is a hyperparameter optimization problem,\n",
    "which we have to evaluate empirically. (In this specific case 40 offers a good tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/dn061sy56s7b4yfppcpnhkqc0000gn/T/ipykernel_39626/2539525735.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "seq_length = 40\n",
    "chunk_size = seq_length+1\n",
    "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded)-chunk_size+1)]\n",
    "from torch.utils.data import Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_chunk = self.text_chunks[index]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x):  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "Tartet (y):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      " Input (x):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "Tartet (y):  'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq,target) in enumerate(seq_dataset):\n",
    "    print(' Input (x): ',repr(\"\".join(char_array[seq])))\n",
    "    print('Tartet (y): ',repr(\"\".join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset,batch_size,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim,rnn_hidden_size,batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size,vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden,cell) = self.rnn(out,(hidden,cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1,batch_size,self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1,batch_size,self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(char_array)\n",
    "EMBED_DIM = 256\n",
    "RNN_HIDDEN_SIZE = 512\n",
    "torch.manual_seed(1)\n",
    "model = RNN(VOCAB_SIZE,EMBED_DIM,RNN_HIDDEN_SIZE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3722\n",
      "Epoch 500 loss: 1.3942\n",
      "Epoch 1000 loss: 1.3521\n",
      "Epoch 1500 loss: 1.2300\n",
      "Epoch 2000 loss: 1.2288\n",
      "Epoch 2500 loss: 1.1846\n",
      "Epoch 3000 loss: 1.1713\n",
      "Epoch 3500 loss: 1.1494\n",
      "Epoch 4000 loss: 1.1892\n",
      "Epoch 4500 loss: 1.1569\n",
      "Epoch 5000 loss: 1.0866\n",
      "Epoch 5500 loss: 1.1185\n",
      "Epoch 6000 loss: 1.1548\n",
      "Epoch 6500 loss: 1.1408\n",
      "Epoch 7000 loss: 1.1057\n",
      "Epoch 7500 loss: 1.1695\n",
      "Epoch 8000 loss: 1.1409\n",
      "Epoch 8500 loss: 1.1615\n",
      "Epoch 9000 loss: 1.1039\n",
      "Epoch 9500 loss: 1.1127\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10_000\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size) # The cell state and hidden state are empty at the beginning of the\n",
    "    # sequence\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred,hidden,cell = model(seq_batch[:,c],hidden,cell)\n",
    "        loss += loss_fn(pred,target_batch[:,c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()/seq_length\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch} loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the next character in the sequence, we can simply select the element with the maximum logit value, which is\n",
    "equivalent to selecting the character with the highest probability. However, instead of always selecting the character\n",
    "with the highest likelihood, we want to (randomly) sample from the outputs (otherwise the model would be deterministic).\n",
    "\n",
    "PyTorch already provides a class, `torch.distributions.Categorical`, which we can use to draw random samples from a\n",
    "categorical distribution (the probabilities represent the probabilities of the element being picked: every element with\n",
    "the same probability means that on a big amount of samples, each element will be picked equally; on element with a\n",
    "larger probability than the others means that that element will be picked more compared to the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "def sample(model, starting_str, len_generated_text = 500,scale_factor=1.0):\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input,(1,-1))\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:,c].view(1),hidden,cell)\n",
    "\n",
    "    last_char = encoded_input[:,-1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1),hidden, cell)\n",
    "        logits = torch.squeeze(logits,0)\n",
    "        scaled_logits = logits*scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island would discover\n",
      "unleady to be feared through the engineer, who, and the sailor’s walls drew a mound, infested without\n",
      "details be allowed themselves, abounted themselves at the open timid fine pottery lengths of these trees of the east following his crims, and\n",
      "has sharp break in Granite House from account of the island, and as soon as CLOUDScouts? The colonists, who put the 10t of the entrance, which searched as the outlet. The day as its apearish answered this lessure, corrived down the two comp\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str=\"The island\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, to control the predictability of the generated samples, the logits computed by the RNN model can be scaled\n",
    "before being passed to `Categorical` for sampling. The scaling factor, $\\alpha$, can be interpreted as an analog to the\n",
    "temperature in physics. Higher temperatures ($\\alpha>1$) result in more entropy or randomness, versus more predictable behaviour at \n",
    "lower temperatures ($\\alpha<1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island of mass, 1 Hove\n",
      "olcaniDest easy ir,\n",
      "dericed off Cyru, fraft which do also howhereable\n",
      "does; I built isffallen have vedntinuous pardoquarting:”\n",
      "\n",
      "Pencroft, dived undly,” as I\n",
      "a leak timic finish!\n",
      "\n",
      "Tubelti’tes instructed lying!--\n",
      "It would necessary,\n",
      "hisip?\n",
      "Ju!”\n",
      "\n",
      "Remember vious up, in side sharp tightly--mysteresaril that-ajours kreased amond, ammunies on wools sharp, precary basalt petuentle being him.-\n",
      "\n",
      "No! kyon-taT--simply. Af anyoks?”\n",
      "\n",
      "“IS” risb-dust!”\n",
      "\n",
      "Cyripdle our mountain widazing?”\n",
      "\n",
      "“clover\n"
     ]
    }
   ],
   "source": [
    "# Higher temperature\n",
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str=\"The island\",scale_factor=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was the colonists had not been able to say another side, and the terrible sources of the island and tools, which was devoted to his companions.\n",
      "\n",
      "At this was there and a spring the pirates and points of the island and a purpose of the summit of the divan, and the sailor was agreed the sand and finding the shore, and and the sailor was more commenced the poultry-yard were then about to the beach. He and Neb knew the presence of the engineer, and Gideon Spilett and his companions disappeared at th\n"
     ]
    }
   ],
   "source": [
    "# Lower temperature\n",
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str=\"The island\",scale_factor=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
