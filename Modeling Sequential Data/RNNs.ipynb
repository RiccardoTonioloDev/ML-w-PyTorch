{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introducing sequential data;\n",
    "- RNNs for modeling sequences;\n",
    "- Long short-term memory;\n",
    "- Truncated backpropagation through time;\n",
    "- Implementing a multilayer RNN for sequence modeling in PyTorch;\n",
    "- Project one: RNN sentiment analysis of the IMDb movie review dataset;\n",
    "- Project two: RNN character-level language modeling with LSTM cells, using text data from a book;\n",
    "- Using gradient clipping to avoid exploding gradients.\n",
    "\n",
    "# Introducing sequential data\n",
    "\n",
    "What makes sequences unique, compared to other types of data, is that elements in a sequence appear in a certain order\n",
    "and are not independent of each other. Typical machine learning algorithms for supervised learning assume that the input\n",
    "is independent and identically distributed (IID) data, which means that the training examples are mutually independent\n",
    "and have the same underlying distribution. Under this constraints the order of the data is irrelevant.\n",
    "\n",
    "With sequences this assumption is not valid.\n",
    "\n",
    "## Sequential data versus time series data\n",
    "\n",
    "Time series data is a special type of sequential data where each example is associated with a dimension for time. Here\n",
    "samples are taken at successive timestamps. For example, stock prices and voice or speech records are time series data.\n",
    "\n",
    "On the other hand, not all sequential data has the time dimensione (like text or DNA).\n",
    "\n",
    "## Representing sequences\n",
    "\n",
    "We need a way to leverage the order information. Throught this chapter, we will represent sequences as $\\langle x^{(1)},\n",
    "...,x^{(T)}\\rangle$, where the length of the sequence is $T$.\n",
    "\n",
    "In a classical NNs we can say that they don't have a memory of previously seen training examples.\n",
    "\n",
    "RNNs, by contrast, are designed for modeling sequences and are capable of remembering pas information and processing\n",
    "new events accordingly, which is a clear advantage when working with sequence data.\n",
    "\n",
    "## The different categories of sequence modeling\n",
    "\n",
    "Sequence modeling has many fascinating applications, such as language transaltion, image captioning and text generation.\n",
    "\n",
    "There are three different relationship categories between input and output data in RNNs:\n",
    "- **Many-to-one**: The input data is a sequence, but the output is a fixed-size vector or scalar, not a sequence. In\n",
    "sentiment analysis the input is text-based and the output is a class label.\n",
    "- **One-to-many**: The input data is in standard format and not a sequence, but the output is a sequence. In image\n",
    "captioning, the input is an image and the output is an English phrase summarizing the content of that image.\n",
    "- **Many-to-many**: Both the input and output array are sequences. This category can be further devided based on whether\n",
    "the input and output are synchronized.\n",
    "    - **Synchronized**: In video classification, each frame is labeled.\n",
    "    - **Delayed**: In language translation, we first read the phrase to be translated, and then we translate.\n",
    "\n",
    "# RNNs for modeling sequences\n",
    "\n",
    "In an RNN, the hidden layer receives its input from both the input layer of the current time step and the hidden layer\n",
    "from the previous time step. The flow of information in adjacent time steps in the hidden layer allows the network to\n",
    "have a memory of past events. This flow of information is usually displayed as a loop, also known as a recurrent edge\n",
    "in graph notation.\n",
    "\n",
    "Similar to MLPs, RNNs can consist of multiple hidden layers. Note that it's a common convention to refer to RNNs with\n",
    "one hidden layer as a single layer RNN.\n",
    "\n",
    "As we know, each hidden unit in a standard NN receives only one input. In contrast, each hidden unit in an RNN receives\n",
    "two distinct sets of input: the preactivation from the input layer and the activation of the same hidden layer from the\n",
    "previous time step, $t-1$.\n",
    "\n",
    "At the first time step, $t=0$, the hidden units are initialized to zeros or small random values. Then, at a time step\n",
    "where $t>0$, the hidden units receive their input from the data point at the current time, $x^{(t)}$, and the previous\n",
    "values of hidden units at $t-1$, indicated as $h^{(t-1)}$.\n",
    "\n",
    "Since, each recurrent layer must receive a sequence as input, all the recurrent layers except the last one must return\n",
    "a sequence as output. The behaviour of the last recurrent layer depends on the type of problem.\n",
    "\n",
    "## Computing activations in an RNN\n",
    "\n",
    "Each directed edge in the representation of an RNN that we just looked at is associated with a weight matrix. Those\n",
    "weights do not depend on time, $t$; therefore, they are shared across the time axis. The different weight matrices in a\n",
    "single-layer RNN are as follows:\n",
    "- $W_{xh}$: The weight matrix between the input and the hidden layer;\n",
    "- $W_{hh}$: The weight matrix associated with the recurrent edge;\n",
    "- $W_{xh}$: The weight matrix between the hidden layer and output layer.\n",
    "\n",
    "Computing the activations is very similar to standard multilayer perceptrons and other types of feedforward NNs. For the\n",
    "hidden layer (in this example we have just one hidden layer), the net input, $z_h$ (preactivation), is computed through\n",
    "a linear combination; that is, we compute the sum of the multiplications of the weight matrices with the corresponding\n",
    "vectors and add the bias unit:\n",
    "$$z_h^{(t)} = W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h$$\n",
    "\n",
    "Then, the activations of the hidden units at the time step, $t$, are calculated as follows:\n",
    "$$h^{(t)} = \\sigma_h(z_h^{(t)}) = \\sigma_h(W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h)$$\n",
    "\n",
    "Once the activations of the hidden units at the current time step are computed, then the activations of the output units\n",
    "will be computed, as follows:\n",
    "$$o^{(t)} = \\sigma_o(W_{ho}h^{(t)}+b_o)$$\n",
    "\n",
    "### Backpropagation Through Time (BPTT)\n",
    "\n",
    "The derivation of the gradients might be a bit complicated, but the basic idea is that the overall loss, $L$, is the sum\n",
    "of all the loss functions at times $t=1$ to $t=T$:\n",
    "$$L=\\sum_{t=1}^{T}L^{(t)}$$\n",
    "\n",
    "Since the loss at time $t$ is dependent on the hiddenunits at all previous time steps $1:t$, the gradient will be\n",
    "computed as follows:\n",
    "$$\\frac{\\partial L^{(t)}}{\\partial W_{hh}} = \\frac{\\partial L^{(t)}}{\\partial o_{(t)}}\\times\\frac{\\partial o^{(t)}}\n",
    "{\\partial h_{(t)}}\\times\\bigl(\\sum_{k=1}^t\\frac{\\partial h^{(t)}}{\\partial h_{(k)}}\\times\\frac{\\partial h^{(k)}}\n",
    "{\\partial W_{(hh)}}\\bigr)$$\n",
    "\n",
    "Here, $\\frac{\\partial h^{(t)}}{\\partial h_{(k)}}$ is computed as a multiplication of adjacent time steps:\n",
    "$$\\frac{\\partial h^{(t)}}{\\partial h_{(k)}} = \\prod_{i=k+1}^t\\frac{\\partial h^{(i)}}{\\partial h_{(i-1)}}$$\n",
    "\n",
    "## Hidden recurrence versus output recurrence\n",
    "\n",
    "Note, that there is a type of model in which the recurrent connection comes from the output layer. In this case, the\n",
    "net activations from the output layer at the previous time step, o^{t-1}, can be added in one of two ways:\n",
    "- The hidden layer at the current time step, h^t (output to hidden recurrence);\n",
    "- The output layer at the current time step, o^t (output to output recurrence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_xj shape:  torch.Size([2, 5])\n",
      "w_hh shape:  torch.Size([2, 2])\n",
      "b_xh shape:  torch.Size([2])\n",
      "b_hh shape:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1)\n",
    "rnn_layer = nn.RNN(input_size=5,hidden_size=2,num_layers=1,batch_first=True)\n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "print(\"w_xj shape: \", w_xh.shape)\n",
    "print(\"w_hh shape: \", w_hh.shape)\n",
    "print(\"b_xh shape: \", b_xh.shape)\n",
    "print(\"b_hh shape: \", b_hh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape for this layer is `(batch_size, sequence_length, timestep_size)` where the first dimension is the batch\n",
    "dimension, the second dimension corresponds to the sequence, and the last dimension corresponds to the features. Notice\n",
    "that we will output a sequence, which, for an input sequence of length 3, will result in an output sequence of length 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "Input:  [[1. 1. 1. 1. 1.]]\n",
      "Hidden:  [[-0.47019297  0.58639044]]\n",
      "Output (manual):  [[-0.35198015  0.52525216]]\n",
      "RNN output:  [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "Input:  [[2. 2. 2. 2. 2.]]\n",
      "Hidden:  [[-0.8888316  1.2364398]]\n",
      "Output (manual):  [[-0.68424344  0.76074266]]\n",
      "RNN output:  [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "Input:  [[3. 3. 3. 3. 3.]]\n",
      "Hidden:  [[-1.3074702  1.8864892]]\n",
      "Output (manual):  [[-0.8649416  0.9046636]]\n",
      "RNN output:  [[-0.8649416  0.9046636]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5,[2.0]*5,[3.0]*5]).float()\n",
    "\n",
    "# Output of the RNN\n",
    "output, hn = rnn_layer(torch.reshape(x_seq,(1,3,5)))\n",
    "\n",
    "# Manually computing the output\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1,5))\n",
    "    print(f'Time step {t} =>\\nInput: ',xt.numpy())\n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh,0,1)) + b_xh\n",
    "    print(f'Hidden: ',ht.detach().numpy())\n",
    "    if t > 0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "    ot = ht + torch.matmul(prev_h,torch.transpose(w_hh,0,1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print(\"Output (manual): \",ot.detach().numpy())\n",
    "    print(\"RNN output: \",output[:,t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperbolic tangent (tanh) activation function is the default in RNNs.\n",
    "\n",
    "## The challenges of learning long-range interactions\n",
    "\n",
    "BPTT, introduces some new challenges. Because of the multiplicative factor $\\frac{\\partial h^{(t)}}{\\partial h_{(k)}}$,\n",
    "in computing the gradients of a loss function, the so-called vanishing and exploding gradient problems arise.\n",
    "\n",
    "Basically $\\frac{\\partial h^{(t)}}{\\partial h_{(k)}}$ has $t-k$ multiplications; therefore, multiplying the weight, $w$,\n",
    "by itself $t-k$ times results in a factor, $w^{t-k}$. As a result, if $|w|<1$, this factor becomes very small when $t-k$\n",
    "is large. On the other hand, if the weight of the recurent edge is $|w| >1$, then $w^{t-k}$ becomes very large when\n",
    "$t-k$ is large. **Note that a large t-k refers to long-range dependencies.** We can see that a naive solution to avoid\n",
    "vanishing or exploding gradients can be reached by ensuring $|w| =1$.\n",
    "\n",
    "There are at least three solutions to this problem:\n",
    "- Gradient clipping: a threshold value is specified for the gradients, and we assign this cut-off value to gradient\n",
    "values that exceed this value.\n",
    "- Truncated backpropagation through time (TBTT): it simply limits the number of time steps that the signal can \n",
    "backpropagate after each forward pass (i.e. even if the sequence has 100 elements, we may only backpropagate the most\n",
    "recent 20 time steps).\n",
    "    - The truncation limits the number of steps that the gradient can effectively flow back and properly update the\n",
    "    weights.\n",
    "- Long Short Term Memory (LSTM).\n",
    "\n",
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "It was introduced to overcome the vanishing gradient problem. The building block of an LSTM is a memory cell, which\n",
    "replaces the hidden layer of standard RNNs.\n",
    "\n",
    "In each memory cell, there is a recurrent edge that has the desirabl weight $w=1$, as we discussed, to overcome the\n",
    "vanishing and exploding gradient problems. The values associated with this recurrent eddge are collectively called the\n",
    "cell state.\n",
    "\n",
    "![LSTM Cell](./LSTM-cell-architecture.png)\n",
    "\n",
    "Notice:\n",
    " - The cell state from the previous time step, $C^{(t-1)}$, is modified to get the cell state at the current time step,\n",
    " $C^{(t)}$, without being multiplied directly by any weight factor;\n",
    " - $h^{(t-1)}$ indicates the hidden units at time $t-1$.\n",
    " - In LSTM there are three different types of gates, which are known as the forget gate, the input gate and the output\n",
    " gate.\n",
    "\n",
    "**The forget gate** ($f_t$):\n",
    "\n",
    "Allows the memory cell to reset the cell state without growing indefinitely. In fact the forget gate decides which\n",
    "information is allowed to go through and which information to suppress. It wasn't a part of the original LSTM cell but\n",
    "was added later to improve the original model.\n",
    "$$f_t = \\sigma(W_{xf}x^{(t)}+W_{hf}h^{(t-1)}+b_f)$$\n",
    "\n",
    "**The input gate** ($i_t$) and the candidate value ($\\tilde{C}_t$):\n",
    "\n",
    "They are responsible for updating the cell state. They are computed as follows:\n",
    "$$i_t = \\sigma(W_{xi}x^{(t)}+W_{hi}h^{(t-1)}+b_i)$$\n",
    "$$\\tilde{C}_t = \\sigma(W_{xc}x^{(t)}+W_{hc}h^{(t-1)}+b_c)$$\n",
    "Then the cell state at time $t$ is computed as follows:\n",
    "$$C^{(t)}=(C^{(t-1)}\\odot f_t)\\oplus(i_t\\odot\\tilde{C}_t)$$\n",
    "\n",
    "**The output gate** ($o_t$):\n",
    "\n",
    "It decides how to update the values of hidden units:\n",
    "$$o_t = \\sigma(W_{xo}x^{(t)}+W_{ho}h^{(t-1)}+b_o)$$\n",
    "\n",
    "Given this, the hidden units at the current time step are computed as follows:\n",
    "$$h^{(t)}=o_t\\odot tanh(C^{(t)})$$\n",
    "\n",
    "### Little explanation of how LSTM achieves not having gradient problems\n",
    "\n",
    "The cell state has a direct path from one time step to the next, meaning that gradients can flow through it with little\n",
    "modification. This directo path is a key factor that prevents the gradients from vanishing over long sequences. It does\n",
    "not undergo the same extensive transformations as the hidden state, so its gradients are less likely to vanish as they\n",
    "pass backward through time.\n",
    "\n",
    "The forget gate determines how much of the previous cell state should be retained in the current cell state. It outputs\n",
    "a value between 0 (forget everything) and 1 (keep everything).\n",
    "- If the forget gate is set to 1, it ensures that the cell state carries forward important information from previous\n",
    "time steps without losing it (this preserves gradients from shrinking);\n",
    "- By controlling how much information is passed forward, the forget gate helps prevent the gradients from growing too\n",
    "large (avoiding accumulation of unnecessary information);\n",
    "\n",
    "The input gate decides how much new information from the current input and previous hidden state should be added to the\n",
    "cell state.\n",
    "The input gate helps prevent the gradient from exploding by limiting the contribution of new information (since large\n",
    "updates to the cell state are avoided).\n",
    "\n",
    "The output gate determines how much of the cell state should be exposed as the hidden state at the current time step.\n",
    "- The output gate ensures that the cell state's information is passed to the hidden state in a controlled manner.\n",
    "- The output gate can prevent the hidden state frombecoming too large by regulating the contribution of the cell state.\n",
    "\n",
    "The sigmoid function uses its extremes (0 and 1) to control the flow of information, while the tanh is used to squash\n",
    "the cell state values, limiting them to a range of -1 to 1.\n",
    "- The outputs of the sigmoid and tanh functions are constrained, meaning that the gradients passed backward through\n",
    "these functions are also constrained, helping prevent the gradients from exploding.\n",
    "\n",
    "In summary, the cell state provides a stable path for gradients to propagate backward through many time steps. The\n",
    "hidden state is more susceptible to vanishing gradients because it involves additional non-linear transformation, but\n",
    "its gradient flow is still stabilized by the gating mechanisms and the control exerted by the forget and input gates.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
