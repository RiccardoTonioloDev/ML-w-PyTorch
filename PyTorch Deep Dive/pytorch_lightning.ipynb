{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning\n",
    "\n",
    "Is a library built on top of the PyTorch library, to simplify the APIs, remove much of the necessary boilerplate code\n",
    "to implement things, and more importantly it allows us to use many advanced features such as **multi-GPU** support and\n",
    "**fast low-precision training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the model\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape = (1,28,28), hidden_units=(32,16)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_acc = Accuracy(task=\"multiclass\",num_classes=10)\n",
    "        self.valid_acc = Accuracy(task=\"multiclass\",num_classes=10)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\",num_classes=10)\n",
    "        # Utilities to automatically compute accuracies\n",
    "\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units:\n",
    "            all_layers.append(nn.Linear(input_size, hidden_unit))\n",
    "            all_layers.append(nn.ReLU())\n",
    "            input_size = hidden_unit\n",
    "\n",
    "        all_layers.append(nn.Linear(hidden_units[-1],10))\n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # Method recognized by lightning\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(self(x),y)\n",
    "        preds = torch.argmax(logits,1)\n",
    "        self.train_acc.update(preds,y)\n",
    "        self.log(\"train_loss\",loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Method recognized by lightning\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train_acc\",self.train_acc.compute())\n",
    "\n",
    "    # Method recognized by lightning\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(self(x),y)\n",
    "        preds = torch.argmax(logits,1)\n",
    "        self.valid_acc.update(preds,y)\n",
    "        self.log(\"valid_loss\",loss, prog_bar=True)\n",
    "        self.log(\"valid_acc\",self.valid_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Method recognized by lightning\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(self(x),y)\n",
    "        preds = torch.argmax(logits,1)\n",
    "        self.test_acc.update(preds,y)\n",
    "        self.log(\"test_loss\",loss, prog_bar=True)\n",
    "        self.log(\"test_acc\",self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data loaders for Lightning\n",
    "\n",
    "There are three ways to prepare the dataset for Lightning:\n",
    "1. Make the dataset part of the model;\n",
    "2. Set up the data loaders as usual and feed them to the `fit` method of a Lightning Trainer;\n",
    "3. Create a `LightningDataModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the LightningDataModule approach\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,data_path=\"../NNs with PyTorch/\"):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToImage(),transforms.ToDtype(torch.float32,scale=True)])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Here we should use this method to download the data and prepare it\n",
    "        # MNIST(root=self.data_path,download=True)\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Prepares the data to making use of it. Can create logic based on the provided stage.\n",
    "        # stage is either 'fit', 'validate', 'test' or 'predict'\n",
    "        mnist_all = MNIST(root=self.data_path,train=True,transform=self.transform,download=False)\n",
    "        self.train, self.val = random_split(mnist_all,[55000,5000],generator=torch.Generator().manual_seed(1))\n",
    "        self.test = MNIST(root=self.data_path,train=False,transform=self.transform,download=False)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train,batch_size=64, num_workers=4,persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val,batch_size=64, num_workers=4,persistent_workers=True)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test,batch_size=64, num_workers=4)\n",
    "\n",
    "# Initializing the data module for training, validation and testing\n",
    "torch.manual_seed(1)\n",
    "mnist_dm = MnistDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model using the PyTorch Lightning Trainer class\n",
    "\n",
    "Lightning implements a `Trainer` class that makes the training model super convenient by taking care of all the\n",
    "intermediate steps, such as calling `.zero_grad()`, `.backward()` and `.step()`. Also, as a bonus, it lets us easily\n",
    "specify one or more GPUs to use (if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:08<00:00, 97.10it/s, v_num=0, train_loss=0.250, valid_loss=0.167, valid_acc=0.936]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:08<00:00, 97.00it/s, v_num=0, train_loss=0.250, valid_loss=0.167, valid_acc=0.936]\n"
     ]
    }
   ],
   "source": [
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "# Using MPS since I'm on apple silicon\n",
    "trainer = pl.Trainer(accelerator=\"mps\",max_epochs=10)\n",
    "\n",
    "trainer.fit(model=mnistclassifier,datamodule=mnist_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model using TensorBoard\n",
    "\n",
    "By default, Lightning tracks the training in a subfolder named `lightning_logs`.\n",
    "\n",
    "Tensorboard can be used to analize the logs using the following commands, targetting the logs directory (during, and\n",
    "after the training has finished).\n",
    "```sh\n",
    "tensorboard --logdir lightning_logs/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
