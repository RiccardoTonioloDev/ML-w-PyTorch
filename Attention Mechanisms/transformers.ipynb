{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Improving RNNs with an attention mechanism;\n",
    "- Introducing the stand-alone self-attention mechanism;\n",
    "- Understanding the original transformer architecture;\n",
    "- Comparing transformer-based large-scale language models;\n",
    "- Fine-tuning BERT for sentiment classification.\n",
    "\n",
    "# Adding an attention mechanism to RNNs\n",
    "\n",
    "Attention mechanis help predictive models to focus on certain parts of the input sequence more than others.\n",
    "\n",
    "Let's consider the traditional RNN model for seq2seq task like language translation, which parses the entire input\n",
    "sequence before producing the translation.\n",
    "\n",
    "The RNN parses teh whole input sentence before producing the first input since, translating a sentence word by word\n",
    "would likely result in grammatical errors. However, on limitation of the seq2seq approach is that the RNN is trying to\n",
    "remember the entire input sequence via one single hidden unit before translating it (this will probably result in a loss\n",
    "of information, especially with long sequences).\n",
    "\n",
    "Thus, similar to how humans translate sentences, it may be beneficial to have access to the whole input sequence at each\n",
    "time step. An attention mechanism lets the RNN access all input elements at each given time step. To avoid making the\n",
    "model overwhelmed by accessing all input sequence elements at each time step, attention mechanisms will assign different\n",
    "attention weights to each input element, resulting in the model focussing on the most relevant elements.\n",
    "\n",
    "## The original attention mechanism for RNNs\n",
    "\n",
    "Given an input sequence, $x$, the attention mechanism assigns a weight to each element $x^{(i)}$ and helps the model\n",
    "identify which part of the input is should focus on.\n",
    "\n",
    "A first RNN is a bidirectional RNN generates context vectors, $c_i$ (augmented version of the input vector,\n",
    "$x^{(i)})$. The context vector also incorporates information from all other input elements via an attention mechanism.\n",
    "\n",
    "Then a second RNN uses this context vector, prepared by the first RNN, to generate the outputs.\n",
    "\n",
    "### Preprocessing the inputs using a bidirectional RNN\n",
    "\n",
    "The RNN#1 processes the input sequence $x$ in the regular forward and backward directions (the backward pass is used to\n",
    "capture additional information since current inputs may have a dependence on sequence elements that came either before \n",
    "or after it in a sentence). \n",
    "\n",
    "Consequently, we have two hidden states for each input sequence element. For instance, for the second input sequence\n",
    "element $x^{(2)}$, we obtain hidden state $h_F^{(2)}$ from the forward pass and the hidden state $h_B^{(2)}$ from the\n",
    "backward pass. These two hidden states are then concatenated in the hidden state $h^{(2)}$. We consider this\n",
    "concatenated hidden state as the \"annotation\" of the source word since it contains the information of the $j$th word in\n",
    "both directions.\n",
    "\n",
    "### Generating outputs from context vectors\n",
    "\n",
    "The RNN#2 is the main RNN that is generating the outputs. In addition to the hidden states, it receives so-called \n",
    "context vectors as input. Think about the context vector as a weighted version of the hidden states obtained from RNN#1:\n",
    "$$c_i = \\sum_{j=1}^T\\alpha_{ij}h^{(j)}$$\n",
    "Here $\\alpha_{ij}$ represents the attention weights over the input sequence, in the context of the $i$th input sequence\n",
    "element. **Note** that each $i$th input sequence element has a unique set of attention weights.\n",
    "\n",
    "Now, RNN#2 receives the aorementioned context vector $c_i$ at each time step $i$ as input. The hidden state $s^{(i)}$\n",
    "depends on the previous hidden state $s^{(i-1)}$, the previous target word $y^{(i-1)}$, and the context vector\n",
    "$c^{(i)}$, which are used to generate the predicted output $o^{(i)}$ for the target word $y^{(i)}$ at time $i$.\n",
    "**Notice** that the RNN#2 doesn't directly use the input sequence $x$.\n",
    "\n",
    "### Computing the attention weights\n",
    "\n",
    "Each attention weight has two subscripts: $j$ refers to the index position of the input and $i$ corresponds to the\n",
    "output index position. The attention weight $a_{ij}$ is a normalized version of the alignment score $e_{ij}$, where the\n",
    "alignmentscore evaluates how well the input around position $j$ matches with the output at position $i$.\n",
    "\n",
    "The attention weight is computed by normalizing the alignment scores ($e$, not Euler's number) as follows:\n",
    "$$\\alpha_{ij}=\\frac{exp(e_{ij})}{\\sum_{k=1}^Te_{ik}}$$\n",
    "\n",
    "Basically is the softmax function, so that the attention weights sum up to 1.\n",
    "\n",
    "> The allignment score can be calculated in various ways, from just the dot product of the two vectors, to doing dot\n",
    "> products of the vectors scaled by learnable matrices.\n",
    "\n",
    "# Introducing the self-attention mechanism\n",
    "\n",
    "We can think of the previously discussed attention mechanism as an operation that connects two different modules, that\n",
    "is, the encoder and decoder of the RNN.\n",
    "Self-attention focuses only on the input and captures only dependencies between the input elements, without connecting\n",
    "two modules.\n",
    "\n",
    "## Starting with a basic form of self-attention\n",
    "\n",
    "Let's assume that we have an input sequence $x$ and an output sequence $z$ ($o$ this time won't be the output of the \n",
    "self-attention mechanism, but will be the output of the model that will use this mechanism).\n",
    "\n",
    "For a seq2seq task, the goal of self-attention is to model the dependencies of the current input element to all other\n",
    "input elements. To achieve this self-attention mechanisms are composed of three stages:\n",
    "1. We derive importance weights based on the similarity between the current element and all other elements in the\n",
    "sequence;\n",
    "2. We normalize the weights, which usually involves using the softmax function;\n",
    "3. We use the weights in combination with the corresponding sequence elements to compute the attention value.\n",
    "\n",
    "More formally, the output of self-attention, $z^{(i)}$, is the weighted sum of all $T$ input sequences, $x$.\n",
    "For instance, for the $i$th input element, the corresponding output value is computed as follows:\n",
    "$$z^{(i)}=\\sum_{j=1}^T\\alpha_{ij}x^{(j)}$$\n",
    "\n",
    "Hence, we can think of $z^{(i)}$ as a context-aware embedding vector in input vector $x^{(i)}$ that involves all other\n",
    "input sequence elements weighted by their respective attention weigths. Here, the attention weights, $\\alpha_{ij}, are\n",
    "computed based on the similarity between the current input element, $x^{(i)}$, and all other elements in the input\n",
    "sequence.\n",
    "\n",
    "### Calculating similarity\n",
    "We compute the dot product between the current input element $x^{(i)}$, and another element in the input sequence\n",
    "$x^{(j)}$:\n",
    "$$\\omega_{ij}=x^{(i)T}x^{(j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 7, 1, 2, 5, 6, 4, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sentence = torch.tensor([0,7,1,2,5,6,4,3])\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's assume that we already encoded this sentence into a rela-number vector representation via an embedding layer\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(10,16)\n",
    "embedded_sentence = embed(sentence).detach()\n",
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now compute omega_ij as the dot product between the ith and the jth word embeddings\n",
    "omega = embedded_sentence @ embedded_sentence.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can obtain the attention weights normalizing the omegas\n",
    "attention_weights = omega.softmax(dim=1)\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each element represents an attention weight. If we are proceesing the $i$ th input word, the $i$ th row of this\n",
    "matrix contains the corresponding attention weights for all words in sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice how they all add up to one\n",
    "attention_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compute the context vectors as the attention-weighted sum of the inputs\n",
    "context_v = attention_weights @ embedded_sentence\n",
    "context_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing the self-attention mechanism: scaled dot-product attention\n",
    "\n",
    "To make the self-attention mechanism more flexible and amenable to model optimization, we will introduce three\n",
    "additional weight matrices that can be fit as model parameters during model training. We denote this three weight\n",
    "matrices as $U_q$, $U_k$ and $U_v$. They are used to preject the inputs into query, key and value sequence elements as\n",
    "follows:\n",
    "- Query sequence $q^{(i)}=U_qx^{(i)}$\n",
    "- Key sequence $k^{(i)}=U_kx^{(i)}$\n",
    "- Value sequence $v^{(i)}=U_vx^{(i)}$\n",
    "\n",
    "> The terms query, key and value that were used in the original transformer paper are inspired by information retrieval\n",
    "> systems and databases. For example, if we enter a query, it is matched against the key values for which values are \n",
    "> retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "d = embedded_sentence.shape[1] # The embedding dimensionality\n",
    "U_query = torch.rand(d,d)\n",
    "U_key = torch.rand(d,d)\n",
    "U_value = torch.rand(d,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16])\n",
      "torch.Size([8, 16])\n",
      "torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Using the query projection matrix, we can compute the query sentence\n",
    "query = embedded_sentence @ U_query\n",
    "\n",
    "# In similar fashion we can compute the query and value sentences\n",
    "key = embedded_sentence @ U_key\n",
    "value = embedded_sentence @ U_value\n",
    "print(query.shape)\n",
    "print(key.shape)\n",
    "print(value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Now lets compute the unnormalized attention weight matrix\n",
    "unn_att_weights = query @ key.T\n",
    "\n",
    "# Now we can normalize it through self attention\n",
    "att_weights = (unn_att_weights/d**0.5).softmax(dim=-1)\n",
    "print(att_weights.shape)\n",
    "print(att_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the scaling by $\\frac{1}{\\sqrt{d}}$, ensures that the euclidean length of the weight vectors will be\n",
    "approximately in the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally we can compute the output by doing the weighted sum of the values (using the attention weights)\n",
    "out = att_weights @ value\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all we need: introducing the original transformer architecture\n",
    "\n",
    "Originally, the intention behind using an attention mechanism was to improve the text generation capabilities of RNNs\n",
    "when working with long sentences. However, reserchers found that an attention-based language model was even more\n",
    "powerful when the recurrent layers were deleted. This led to the transformer architecture.\n",
    "\n",
    "<img src=\"./attention_research_1.webp\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding contxt embedding via multi-head attention\n",
    "\n",
    "The overall goal of the encoder block is to take in a sequential input $X$ and map it into a continuous representation\n",
    "$Z$ that is then passed on to the decoder.\n",
    "\n",
    "The encoder is a stack of six (the hyperparameter that better suited the performances of the architecture, in the \n",
    "original paper) identical layers. Inside of each of the layers there are two sublayers: one computes multi-head\n",
    "self-attention, and the other one is a fully connected layer.\n",
    "\n",
    "Multi-head self-attention is a simple modification of the scaled dot-product attention covered earlier. In the context\n",
    "of multi-head attention, we can think of the set of query, key and value matrices as one attention head. As indicated by\n",
    "the name, in multi-head attention, we now have multiple of such heads, similar to how convolutional neural networks can\n",
    "have multiple kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling:\n",
      "torch.Size([8, 8, 16])\n",
      "torch.Size([8, 8, 16])\n",
      "torch.Size([8, 8, 16])\n",
      "\n",
      "Attention weights:\n",
      "torch.Size([8, 8, 8])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "\n",
      "Weighted sum:\n",
      "torch.Size([8, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# To demonstrate the concept with code\n",
    "torch.manual_seed(123)\n",
    "h = 8\n",
    "d = embedded_sentence.shape[1]\n",
    "multi_U_query = torch.rand(h,d,d)\n",
    "multi_U_key = torch.rand(h,d,d)\n",
    "multi_U_value = torch.rand(h,d,d)\n",
    "\n",
    "# Scaling vectors\n",
    "multi_query = embedded_sentence @ multi_U_query\n",
    "multi_key = embedded_sentence @ multi_U_key\n",
    "multi_value = embedded_sentence @ multi_U_value\n",
    "\n",
    "print(\"Scaling:\")\n",
    "print(multi_query.shape)\n",
    "print(multi_key.shape)\n",
    "print(multi_value.shape)\n",
    "\n",
    "# Computing attention\n",
    "unn_att_weights = multi_query @ multi_key.permute((0,2,1))\n",
    "att_weights = (unn_att_weights / d**0.5).softmax(dim=-1)\n",
    "\n",
    "print(\"\\nAttention weights:\")\n",
    "print(att_weights.shape)\n",
    "print(att_weights.sum(dim=-1))\n",
    "\n",
    "# Weighted sum with attention\n",
    "out = att_weights @ multi_value\n",
    "print(\"\\nWeighted sum:\")\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "# Now we just have to concatenate attention heads\n",
    "out = out.permute(1,0,2).reshape(embedded_sentence.shape[0],h*d)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "# After concatenation we have to map back the output of the multi head attention mechanism to a single vector of\n",
    "# dimensionality d for each element of the sequence\n",
    "linear = torch.nn.Linear(h*d,d)\n",
    "context_vector = linear(out)\n",
    "print(context_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if multi-head attention sounds computationally expensive, notice that the computation can all be done in parallel\n",
    "because there are no dependencies between the multiple heads.\n",
    "\n",
    "## Learning a language model: decoder and masked multi-head attention\n",
    "\n",
    "Similar to the encoder, the decoder also contains several repreated layers. Besides the two sublayers that we have\n",
    "already introduced in the previous encoder section, each repeated layer also contains a masked multi-head attention\n",
    "sublayer.\n",
    "\n",
    "Masked attention is a variation of the original attention mechanism, where masked attention only passes a limited input\n",
    "sequence into the model by \"masking\" out a certain number of words. For example, if we are building a language\n",
    "translation model with a labeled dataset, at sequence position $i$ during the training procedure, we only feed in the\n",
    "correct output words from positions $1,...,i-1$. All other words are hidden from the model to prevent the model from\n",
    "cheating.\n",
    "\n",
    "The encoder works like this:\n",
    "1. It receives the otput embeddings and gives them to the first masked multi-head attention layer;\n",
    "2. Then the second layer receives both encoded inputs from the encoder block and the output of the masked multi-head\n",
    "attention layer;\n",
    "3. Finally, we pass the multi-head attention outputs into a fully connected layer that generates the overall model\n",
    "output (a probability vector corresponding to the output words).\n",
    "\n",
    "Comparing the decoder with the encoder block, the main difference is the range of sequence elements that the model can\n",
    "attend to. In the encoder, for each given word, the attention is calculated across al the words in a sentence, whiich\n",
    "can be considered as a form of bidirectional input parsing. The decoder also receives the bidirectionally parsed inputs\n",
    "from the encoder. However, when it comes to the output sequence, the decoder only considers those elements that are\n",
    "preceding the current input position, which can be interpreted as a form of unidirectional input parsing.\n",
    "\n",
    "## Implementation details: positional encodings and layer normalization\n",
    "\n",
    "Positional encodings help with capturing information about the input sequence ordering and are a crucial of transformers\n",
    "because both scaled dot-product attention layers and fully connected layers are permutation-invariant (without\n",
    "positional encoding, the order of words is ignored and does not make any difference to the attention-based encodings).\n",
    "\n",
    "Transformers enable the same words at different positions to have slightly different encodings by adding a vector of\n",
    "small values to the input embedding at the beginning of the encoder and decoder blocks. In the context of the original\n",
    "transformer paper they used the so-called sinusoidal encoding:\n",
    "$$\n",
    "PE_{(i,2k)} = sin(pos/10000^{2k/d_{model}}) \\\\\n",
    "PE_{(i,2k+1)} = cos(pos/10000^{2k/d_{model}})\n",
    "$$\n",
    "\n",
    "Here $i$ is the position of the word and $k$ denotes the length of the encoding vector, where we choose $k$ to have the \n",
    "same dimension as the input word embedding so that the positional encoding and word embeddings can be added together.\n",
    "\n",
    "In general, there are two types of positional encodings, an absolute one (as shown in the previous formula) and a\n",
    "relative one. Absolute positional encodings are fixed vectors for each given position, while relative encodings only\n",
    "maintain the relative position of words and are invariant to sentence shift.\n",
    "\n",
    "Next, let's look at the layer normalization mechanism. While batch normalization, is a popular choice in computer vision\n",
    "contexts, layer normalization is the preferred choice in NLP contexts, where sentence lengths can vary.\n",
    "\n",
    "While layer normalization is traditionally performed across all elements in a given feature for each feature\n",
    "indipendently (the focus is on each feature for all the training examples), the layer normalization used in transformers\n",
    "extends this concept and computes the normalization statistics across all feature value indipendently for each training\n",
    "example (the focus is on one training example for all its features).\n",
    "\n",
    "Since layer normalization computes mean and standard deviation for each training example, it relaxes minibatch size\n",
    "constraints or dependencies. In contrast to batch normalization, layer normalization is thus capable of learning from\n",
    "data with small minibatch sizes and varying lengths.\n",
    "\n",
    "However, note that the transformer architecture does not have varying-length inputs (sequences are padded when needed),\n",
    "and there is no recurrence in the model. Transformers are usually trained on very large text corpora, which requires\n",
    "parallel computation. This can be challenging to achieve with batch normalization, which has a dependency between\n",
    "training examples (layer normalization has no such dependency).\n",
    "\n",
    "# Building large-scale language models by leveragin unlabeled data\n",
    "\n",
    "One common thing about large-scale transformers is that they are pre-trained on very large, unlabeled datasets and then\n",
    "fine-tuned for their respective target tasks.\n",
    "\n",
    "## Pre-training and fine-tuning transformer models\n",
    "\n",
    "Language translation is a supervised task and requires a labeled dataset, which can be very expensive to obtain. The\n",
    "lack of large, labeled datasets is a long-lasting problem in deep learning, especially for models like the transformer,\n",
    "which are even more data hungry than other deep learning architectures. However, given that large amounts of text are\n",
    "generated every day, an interesting question is how we can use such unlabeled data for improving the model training.\n",
    "\n",
    "We can generate \"labels\" for supervised learning from plain text itself, for example doing a next-word prediction task.\n",
    "This enables the model to learn the probability distribution of words and can form a strong basis for becoming a\n",
    "powerful language model. This technique is called unsupervised pre-training.\n",
    "\n",
    "The main idea of pre-training is to make use of plain text and then transfer and fine-tune the model to perform some\n",
    "specific tasks for which a (smaller) labeled dataset is available. Now, there are many pre-training techniques (the next\n",
    "word prediction technique can be thought of as a unidirectional pre-training approach).\n",
    "\n",
    "With the representations that can be obtained from the pre-trained model, there are mainly two strategies for\n",
    "transferring and adopting a model to a specific task:\n",
    "1. Feature-based approach: it uses pre-trained representations as additional features to a labeled dataset. This \n",
    "requires us to learn how to extract sentence features from the pre-trained model. In other word, we can think of the\n",
    "feature-based approach as a model-based feature extraction technique similar to principal component analysis;\n",
    "2. Fine-tuning approach: updates the pre-trained model parameters in a regular supervised fashion via backpropagation.\n",
    "Unlike the feature-based method, we usually also add another fully connected layer to the pre-trained model, to\n",
    "accomplish certain tasks such as classification, and then update the whole model based on the prediction performance on\n",
    "the labeled training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT-2 to generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation',model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The best stock to have in your portfolio is that you want to put a good amount of the price right into that asset, so that's where you want the best performance.\\n\\nRounding out the list above are investments based primarily on short term\"}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(12)\n",
    "generator(\"The best stock to have in your portfolio is\",max_length=50,truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional pre-training with BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) has a transformer-encoder-based model structure that\n",
    "utilizes bidirectional training procedure (it reads in all input elements all at once). This kind of pretraining\n",
    "disables BERT's ability to generate a sentence word by word but provides input encodings of higher quality for other\n",
    "tasks, such as classification, since the model can now process information in both directions.\n",
    "\n",
    "Recall that in a transformer's encoder, token encoding is a summation of positional encodings and token embeddings. In\n",
    "the BERT encoder, theere is an additional segment embedding indicating which segment this token belongs to. Why do we\n",
    "need this additional segment information in BERT? The need for this segment information originated from the special\n",
    "pre-training task of BERT called *next-sentence-prediction*. In this pre-training task, each training example includes\n",
    "two sentences and thus requires special segment notation to denote whether it belongs to the first or second sentence.\n",
    "\n",
    "The pre-training of BERT includes two unsupervised tasks: masked language modeling and next-sentence prediction.\n",
    "\n",
    "In the masked language model, tokens are randomly replaced by so-called mask tokens, and the model is required to\n",
    "predict these hidden words. With this technique BERT is more akin to \"filling the blanks\" because the model can attend\n",
    "to all tokens in the sentence. To compensate the fact that MASK tokens don't usually appear in regular texts, there are\n",
    "further modificatinos to the words that are selected for masking: 15% of the words in BERT are marked for masking, then\n",
    "15% of randomly selected words are then further treated as follows:\n",
    "1. Keep the word unchanged 10% of the time;\n",
    "2. Replace the original word token with a random word 10% of the time;\n",
    "3. Replace the original word token with a mask token 80% of the time.\n",
    "\n",
    "(This distribution of probabilities was the best among all the tested ones in the original paper)\n",
    "\n",
    "Other than partially solving the aforementioned problem, these modifications also have other benefits. Firstly,\n",
    "unchanged words include the possibility of maintaining the information of the original token (otherwise the model can\n",
    "only learn from the context and nothing from the masked words). Secondly the 10% random words prevent the model from\n",
    "becoming lazy (i.e. learning nothing but returning what it is being given).\n",
    "\n",
    "Talking about the next-sentence prediction task, the model is given two sentences, A and B, in the following format:\n",
    "$$[CLS] A [SEP] B [SEP]$$\n",
    "Where $[CLS]$ is a classification token, which serves as a placeholder or the predicted label in the decoder output, as\n",
    "well as a token denoting the beginning of the sentences. The $[SEP]$ token, on the other hand, is attached to denote the\n",
    "end of each sentence. The model is then required to classify whether B is the next sentence of A or not.\n",
    "\n",
    "To provide the model with a balanced dataset, 50% of the samples are labeled as \"IsNext\" while the remaining samples are\n",
    "labeled as \"NotNext\".\n",
    "\n",
    "The training objective of BERT is to minimize the combined loss function of both tasks.\n",
    "\n",
    "> NOTE: each input example needs to match a certain format. For example, it should begin with a $[CLS]$ token and be\n",
    "> separed using $[SEP]$ tokens if it consists of more than one sentence.\n",
    "\n",
    "BERT can be then fine-tuned on four categories of tasks:\n",
    "1. Sentence pair classification;\n",
    "2. Single sentence classification;\n",
    "3. Question answering;\n",
    "4. Single-sentence tagging.\n",
    "\n",
    "The first two only require an additional softmax layer to be added to the output representation of the $[CLS]$ token.\n",
    "The last two, however, are token-level classification tasks, which means that the model passes output representations of\n",
    "all related tokens to the softmax layer to predict a class label for each individual token.\n",
    "\n",
    "# The bestof both worlds: BART\n",
    "\n",
    "The Bidirectional and Auto-Regressive Transformer (BART) can be viewed as a generalization of both GPT and BERT. As the\n",
    "title of this section suggests, BART is able to accomplish both tasks, generating and classifying text. The reason\n",
    "resides in the model having both a bidirectional encoder as well as a left-to-right autoregressive decoder.\n",
    "\n",
    "One of the more interesting changes is that BART works with different model inputs. In BART the input format was\n",
    "generalized such that it only uses the source sequence as input. Upon receiving a training example as plain text, the\n",
    "input will first be \"corrupted\" and then encoded by the encoder. These input encodings will then be passed to the\n",
    "decoder, along with the generated tokens. The cross-entropy loss between encoder output and the original text will be\n",
    "calculated and then optimized through the learning process.\n",
    "\n",
    "# Fine-tuning a BERT model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification # (Smaller distilled version of BERT)\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device(\"mps\")\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Sentiment analysis/movie_data.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the tokenizer used for the training of the BERT model in order to tokenize the new text\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts),truncation=True,padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts),truncation=True,padding=True)\n",
    "test_encodings = tokenizer(list(test_texts),truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=16,shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(),lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids,attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predicted_labels = torch.argmax(logits,1)\n",
    "            num_examples+=labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "    return correct_pred.float()/num_examples*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Preparing data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        # Backward pass\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Logging\n",
    "        if not batch_idx % 250:\n",
    "            print(\n",
    "                f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} \\\n",
    "                | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}'\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            print(f'Training accuracy: {compute_accuracy(model,train_loader,DEVICE):.2f}% \\\n",
    "                  Valid accuracy: {compute_accuracy(model,valid_loader,DEVICE):.2f}%')\n",
    "        print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f\"Total Training Time: {(time.time() - start_time)/60:.2f} min\")\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader,DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another more convenient way of training the code it though the `Trainer` API.\n",
    "\n",
    "The preceding (and the code of the rest of the section) code isn't executed because I don't have a GPU to use for the\n",
    "fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
