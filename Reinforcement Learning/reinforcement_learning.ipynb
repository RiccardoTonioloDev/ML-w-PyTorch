{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning the basics of RL;\n",
    "- Indroducing different categories of RL problems;\n",
    "- Implementing a Q-learning algorithm in a tabular format;\n",
    "- Understanding function approximation for solving RL problems and implementing a deep Q-learning algorithm.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The key element that distinguishes RL from other subtaks of machine learning, such as supervised and unsupervised\n",
    "learning, is that RL is centered around the concept of learning by interaction (the model learns from iteractions with\n",
    "an environment to maximize a reward function).\n",
    "\n",
    "With RL, the model (also called agent) interacts with its environment, and by doing so generates a sequence of\n",
    "interactions that are together called an episode. Through these interactions, the agent collects a series of rewards\n",
    "determined by the environment. These rewards can be positive or negative, and sometimes thay are not disclosed to the\n",
    "agent until the end of an episode.\n",
    "\n",
    "## Defining the agent-environment interface of a reinforcement learning system\n",
    "\n",
    "There are two distinct entities: an agent and an environment.\n",
    "\n",
    "Formally, an agent is defined as an entity that learns how to make decisions and interacts with its surrounding\n",
    "environment by taking an action. The environment is anything that falls outside the agent, and it communicates with the\n",
    "agent, determining the reward signal for the agent's action.\n",
    "\n",
    "The reward signal is the feedback that the agent receives from interacting with the environment, which is usually\n",
    "provided in the form of a scalar value an can be either positive or negative.\n",
    "\n",
    "During the learning process,the agent must try different actions (exploration) so that it can progressively learn which\n",
    "actions to prefer and perform more often (exploitation) in order to maximize the total, cumulative reward.\n",
    "In general, exploitation will result in choosing actions with a greater short-term reward, whereas exploration can\n",
    "potentially result in greater total rewards in the long run. The tradeoff between the two has been studied extensively,\n",
    "but there's no universal answer to this decision-making dilemma.\n",
    "\n",
    "# The theoretical foundations of RL\n",
    "\n",
    "## Markov decision processes\n",
    "\n",
    "In general, the type of problems that RL deals with are typically formulated as Markov decision processes (MDPs), where\n",
    "the standard approach for solving them is by using dynamic programming. \n",
    "\n",
    "However, RL offers some key advantages over dynamic programming. Dynamic programming is not a feasable approach when the\n",
    "size of states (the number of possible configurations) is relatively large. In such cases, RL is considered a much more\n",
    "efficient and practical approach.\n",
    "\n",
    "### The mathematical formulation of Markov decision processes\n",
    "\n",
    "The types of problems that require learning an interactive and sequential decision-making process, where the decision at\n",
    "time step $t$ affects the subsequent situations, are mathematically formalized as MDPs.\n",
    "\n",
    "In the case of the agent/environment interactions in RL, if we denote the agent's starting state as $S_0$, the\n",
    "interactions between the agent and the environment result in a sequence as follows:\n",
    "$$\\{S_0,A_0,R_1\\},\\{S_1,A_1,R_2\\},\\{S_2,A_2,R_3\\},...$$\n",
    "\n",
    "Here, $S_t$ and $A_t$ stand for the state and the action taken at time step $t$. $R_{t+1}$ denotes the reward received\n",
    "from the environment after performing action $A_t$. Note that those variables are time dependent random variables that\n",
    "take values from predefined finite sets denoted by $s\\in\\hat{S},r\\in\\hat{R},a\\in\\hat{A}$ respectively.\n",
    "\n",
    "In an MDP, these time-dependent random variables, $S_t$ and $R_{t+1}$, have probability distributions that only depend\n",
    "on their values at the preceding time step $t-1$. The probability distribution for $S_{t+1} = s'$ and $R_{t+1} = r$ can\n",
    "be written as a conditional probability over the preceding state ($S_t$) and taken action ($A_t$) as follows:\n",
    "$$p(s',r|s,a) =: P(S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a)$$\n",
    "\n",
    "This probability distribution completely defines the dynamics of the environment because, based on this distribution,\n",
    "all transition probabilities of the environment can be computed. Therefore, the environment dynamics are a central\n",
    "criterion for categorizing different RL methods. The types of RL methods that requirea model of the environment or try\n",
    "to learn a model of the environment (that is, the environment dynamics) are called model-based methods (as opposed to\n",
    "model-free methods).\n",
    "\n",
    "When the probability $p(s',r|s,a)$ is known, then the learning task can be solved with dynamic programming. But when the\n",
    "dynamics of the environment are not known, as is the case in many real-world problems, then we would need to acquire a\n",
    "large number of samples by interacting with the environment to compensate for the unkown environment dynamics. Two main\n",
    "approaches for dealing with this problem are the model-free Monte Carlo (MC) and temporal difference (TD) methods.\n",
    "\n",
    "The environment dynamics can be considered deterministic if particular actions for given states are always or never\n",
    "taken, that is, $p(s',r|s,a)\\in\\{0,1\\}$. Otherwise, in the more general case, the environment would have a stochastic\n",
    "behaviour.\n",
    "\n",
    "To make sense of this stochastic behaviour, let's consider the probability of observing the future state $S_{t+1}=s'$\n",
    "conditioned on the current state $S_t=s$ and the performed action $A_t=a$. It can be computed as a marginal probability\n",
    "by taking the sum over all possible rewards:\n",
    "$$p(s'|s,a) =: \\sum_{r\\in\\hat{R}}p(s',r|s,a)$$\n",
    "\n",
    "This probability is called state-transmission probability (the probability of transitioning to that specific state) and\n",
    "based on it, if the environment dynamics are deterministic, then it means that when the agent takes action $A_t=a$ at\n",
    "state $S_t=s$, the transition to the next state, $S_{s+1}=s'$, will be 100 percent certain, that is, $p(s'|s,a) = 1$.\n",
    "\n",
    "## Visualization of a Markov process\n",
    "\n",
    "A Markov process can be represented as a directed cyclic graph in which the nodes in the graph represent the different\n",
    "states of the environment. The edges of the graph represent transition probabilities between the states. Obviously the \n",
    "transition probabilities coming out of each state (node) always sum to 1.\n",
    "\n",
    "## Episodic versus continuing tasks\n",
    "\n",
    "As the agent interacts with the environment, the sequence of observations or states forms a trajectory.\n",
    "- If an agent's trajectory can be divided into subparts such that each starts at time $t=0$ and ends in a terminal state\n",
    "$S_t$ (at $t=T$), the task is called an episodic task (learn an agent for the game of chess).\n",
    "- On the other hand, if the trajectory is infinitely continuous without a terminal state, the task is called a\n",
    "continuing task (cleaning robot that has to keep the house tidy).\n",
    "\n",
    "In episodic tasks, an episode is a sequence or trajectory that an agent takes from a starting state, $S_0$, to a\n",
    "terminal state, $S_t$.\n",
    "\n",
    "## RL terminology: return, policy and value function\n",
    "\n",
    "### The return\n",
    "\n",
    "The return at time $t$ is the cumulated reward obtained from the entire duration of an episode. Recall that $R_{t+1}=r$\n",
    "is the immediate reward obtained after performing an action, $A_t$ at time $t$.\n",
    "\n",
    "The return at time $t$ can then be calculated as follows:\n",
    "$$G_t=: R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}\\gamma^kR_{t+k+1}$$\n",
    "\n",
    "Here $\\gamma$ is the discount factor in range $[0,1]$. The parameter $\\gamma$ indicates how much the future rewards are\n",
    "\"worth\" at the current moment (time $t$). By setting $\\gamma=0$ for example, we are saying that we do not care about\n",
    "future rewards.\n",
    "\n",
    "### Policy\n",
    "\n",
    "A policy typically denoted by $\\pi(a|s)$ is a function that determines the next action to take, which can be either\n",
    "deterministic or stochastic. A stochastic policy then has a probability distribution over actions that an agent can take\n",
    "at a given state:\n",
    "$$\\pi(a|s) =: P[A_t=a|S_t=s]$$\n",
    "\n",
    "During the learning process, the policy may change as the agent gains more experience. For example, the agent may start\n",
    "from a random policy, where the probability of all actions is uniform; meanwhile, the agent will hopefully learn to\n",
    "optimize its policy toward reaching the optimal policy. The optimal policy $\\pi_*(a|s)$ is the policy that yields the\n",
    "highest return.\n",
    "\n",
    "### Value function\n",
    "\n",
    "The value function, also referred to as the state-value function, measures the goodness of each state (how good or bad\n",
    "it is to be in a particular state)(note that the criterion for goodness is based on the return).\n",
    "\n",
    "Now, based on the return $G_t$, we define the value function of state $s$ as the expected return (the average return\n",
    "over all possible episodes) after following policy $\\pi$:\n",
    "$$v_\\pi(s) =: E_\\pi[G_t|S_t=s] = E_\\pi\\bigl[\\sum_{k=0}\\gamma^{k+1}R_{t+k+1}|S_t=s\\bigr]$$\n",
    "\n",
    "Moreover, we can also define a value for each state-action pair, which is called the action value function and is\n",
    "denoted by $q_\\pi(s,a)$. The action-value function refers to the expected return $G_t$ when the agent is at state\n",
    "$S_t=s$ and takes action $A_t=a$. Extending this definition of the state-value function to state-action pairs, we get\n",
    "the following:\n",
    "$$q_\\pi(s,a) =: E_\\pi[G_t|S_t=s,A_t=a] = E_\\pi\\bigl[\\sum_{k=0}\\gamma^{k+1}R_{t+k+1}|S_t=s,A_t=a\\bigr]$$\n",
    "\n",
    "This is similar to referring to the optimal policy as $\\pi_*(a|s)$, $v_*(s)$ and $q_*(s,a)$ also denote the optimal\n",
    "state-value and action-value functions.\n",
    "\n",
    "Estimating the value function is an essential component of RL methods.\n",
    "\n",
    "### The difference between the reward, return and value function\n",
    "\n",
    "The reward is a consequence of the agent taking an action given the current state of the environment (the agent receives\n",
    "it when performing an acction to transition from one state to the next (not always, sometimes rewards are given only at\n",
    "the end of the episodes)).\n",
    "\n",
    "To measure how good or bad the state is, the value function comes into play. Typically, the states with a \"good\" value\n",
    "are those states that have a high expected return and will likely yield a high reward given a particular policy (i.e. if\n",
    "the computer captures the queen of the opponent in a game of chess, withouth bad consequences, the reward won't arrive\n",
    "until the end of the game, but having captured the queen would result in a state with \"high\" value (since most of the\n",
    "times capturing the queen end up in winning (maximizing rewards))).\n",
    "\n",
    "The return is just a weighted sum of the rewards for an entire episode, which would be equal to the discounted final\n",
    "reward in our chess example. So the value function is the expectation over all possible episodes, which basically\n",
    "computes how \"valuable\" it is, on average, to make a certain move.\n",
    "\n",
    "## Dynamic programming using Bellman equation\n",
    "\n",
    "The Bellman equation simplifies the computation of the value function, such that rather than summing over multiple time\n",
    "steps, it uses a recursion that is similar to the recursion for computing the return.\n",
    "\n",
    "Based on the recursive equation for the total return $G_t=r+\\gamma G_{t+1}$, we can rewrite the value function as\n",
    "follows:\n",
    "$$v_\\pi(s)=:E_\\pi[G_t|S_t=s]=E_\\pi[r+\\gamma G_{t+1}|S_t=s]=r+\\gamma E_\\pi[G_{t+1}|S_t=s]$$\n",
    "\n",
    "Similarly we could rewrite the action-value function as:\n",
    "$$q_\\pi(s,a)=:E_\\pi[G_t|S_t=s,A_t=a]=E_\\pi[r+\\gamma G_{t+1}|S_t=s,A_t=a]=r+\\gamma E_\\pi[G_{t+1}|S_t=s,A_t=a]$$\n",
    "\n",
    "We can use the environment dynamics to compute the expectation by summing all the probabilities of the next state\n",
    "$s'$ and the corresponding rewards $r$:\n",
    "$$v_\\pi(s)=\\sum_{a\\in\\hat{A}}\\pi(a|s)\\sum_{s'\\in\\hat{S},r\\in\\hat{R}} p(s',r|s,a)[r + E_\\pi[\\gamma G_{t+1}|S_{t+1}=s']]$$\n",
    "\n",
    "Now, we can see tat the expectation of the return, $E_\\pi[G_{t+1}|S_t=s']$, is essentially the state-value function\n",
    "$v_\\pi(s')$. So, we can write $v_\\pi(s)$ as a function of $v_\\pi(s')$:\n",
    "$$v_\\pi(s)=\\sum_{a\\in\\hat{A}}\\pi(a|s)\\sum_{s'\\in\\hat{S},r\\in\\hat{R}} p(s',r|s,a)[r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "This is called the Bellman equation, which relates the value function for a state, $s$, to the value function of its\n",
    "subsequent state, $s'$. This greatly simplifies the computation of the value function because it eliminates the\n",
    "iterative loop along the time axis.\n",
    "\n",
    "Basically with that equation (if environment dynamics were available) we can compute the \"goodness\" of every possible\n",
    "state, so that we could create an optimal strategy through dynamic programming. You start from the final state, $s_F$,\n",
    "and compute backwards. However this could be not doable if the system is too complex, because we should consider too\n",
    "many different states (i.e. think about the game of chess).\n",
    "\n",
    "# Reinforcement learning algorithms\n",
    "\n",
    "## Dynamic programming\n",
    "\n",
    "We'll focus on solving RL problems under the following assumptions:\n",
    "- We have full knowledge of the environment dynamics; that is, all transition probabilities $p(s',r|s,a)$ are known.\n",
    "- The agent's state has the Markov property, which means that the next action and reward depend only on the current\n",
    "state and the choice action we make at this moment or current time step.\n",
    "\n",
    "We should emphasize that dynamic programming is not a practical approach for solving RL problems. The problem with\n",
    "using dynamic programming is that it assumes full knowledge of the environment dynamics, which is usually unreasonable\n",
    "or impractical for most real-world applications.\n",
    "\n",
    "There are two main objectives via the following described tasks:\n",
    "1. Obtain the true state-value function, $v_\\pi(s)$; this task is also known as the prediction task and is accomplished\n",
    "with policy evaluation;\n",
    "2. Find the optimal value function, $v_*(s)$, which is accomplished via generalized policy iteration.\n",
    "\n",
    "### Policy evaluation - predicting the value function with dynamic programming\n",
    "\n",
    "Based on the Bellman equation, we can compute the value function for an arbitrary policy $\\pi$ with dynamic programming\n",
    "when the environment dynamics are known. For computing this value function , we can adapt an iterative solution, where\n",
    "we start from $v^{\\langle i \\rangle}$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
