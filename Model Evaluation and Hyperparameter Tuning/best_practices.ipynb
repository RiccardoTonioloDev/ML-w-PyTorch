{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will learn to:\n",
    "- Assess the performance of machine learning models;\n",
    "- Diagnose the common problems of machine learning algorithms;\n",
    "- Fine-tune machine learning models;\n",
    "- Evaluate predictive models using different performance matrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'M'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = df.loc[:,2:].values\n",
    "y = df.loc[:,1].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform([\"M\",\"B\"]) # Malign tumors as class 1 and benign tumors as class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will chain the scaling, the dim-reduction and the classifier training in a pipeline as a best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe_lr = make_pipeline(StandardScaler(),PCA(n_components=2),LogisticRegression())\n",
    "pipe_lr.fit(X_train,y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "test_acc = pipe_lr.score(X_test,y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_pipeline` function comes in handy, taking an arbitrary number of transformers (objects that support both `fit` and `transform` methods), followed by an estimator (supports `fit` and `predict` methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross-validation to assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The holdout method\n",
    "\n",
    "Basically separating the training set and the test set, in order to use the former for training and the latter to estimate the generalization performance.\n",
    "\n",
    "To select optimal values of tuning parameters, we go through a process called **model selection**.\n",
    "However if we perform the model selection process using everytime the same test set, it's like if the test set becomes a part of the training set (so it's not really a good practice).\n",
    "\n",
    "A better way of using the holdout method for model selection is to separate the data into three parts: training, test and validation dataset. Then the validation set is used for model selection, while the test set is used as the final performance estimate (after the model selection process is complete)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "In k-fold cross validation we randomly split the training dataset into $k$ folds without replacement. This will create $k-1$ training folds, used for training and $1$ test fold, used for performance evaluation.\n",
    "The procedre is repeated $k$ times so that we obtain $k$ models and performance estimates.\n",
    "Typically, we use k-fold cross-validation for model tuning (finding the optimal hyperparameter values that yield a satisfying generalization performance, on the test folds). After we have found satisfactory hyperparameter values, we can retrain the model on the complete training dataset and obtain a final performance estimate using the indipendent test dataset.\n",
    "\n",
    "Another cool property of this approach is that the estimated performances $E_i$ of each model (coming from each iteration of the k-fold) can be then used to calculate the estimated average performance $E= \\frac{1}{k}\\sum^k_{i=1}E_i$.\n",
    "\n",
    "A standard vale for $k$ in k-fold cross validation is usually 10 (as empirical evidence shows). However with relatively small training sets, it can be useful to increase the number of folds (however big values of $k$ will increase the runtime of the cross-validation), while if the training set is big, we can choose a lower $k$ (for example $k=5$), and still obtain an accurate estimate.\n",
    "\n",
    "With extremely small datasets there is also the **leave one out approach** where $k=n$, having only 1 sample for the test set.\n",
    "\n",
    "In case of class imbalances however, a better approach is the stratified cross-validation, where the class label proportions are preserved in each fold, to ensure that each fold is representative of the class proportions in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 01, Class distr.: [256 153], Acc.: 0.935\n",
      "Fold: 02, Class distr.: [256 153], Acc.: 0.935\n",
      "Fold: 03, Class distr.: [256 153], Acc.: 0.957\n",
      "Fold: 04, Class distr.: [256 153], Acc.: 0.957\n",
      "Fold: 05, Class distr.: [256 153], Acc.: 0.935\n",
      "Fold: 06, Class distr.: [257 153], Acc.: 0.956\n",
      "Fold: 07, Class distr.: [257 153], Acc.: 0.978\n",
      "Fold: 08, Class distr.: [257 153], Acc.: 0.933\n",
      "Fold: 09, Class distr.: [257 153], Acc.: 0.956\n",
      "Fold: 10, Class distr.: [257 153], Acc.: 0.956\n",
      "\n",
      "CV accuracy: 0.950 +/- 0.014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train,y_train)\n",
    "scores = []\n",
    "for k, (train,test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train],y_train[train])\n",
    "    score = pipe_lr.score(X_train[test],y_train[test])\n",
    "    scores.append(score)\n",
    "    print(f\"Fold: {k+1:02d}, Class distr.: {np.bincount(y_train[train])}, Acc.: {score:.3f}\")\n",
    "mean_acc = np.mean(scores)\n",
    "std_acc = np.std(scores)\n",
    "print(f\"\\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.93478261 0.93478261 0.95652174 0.95652174 0.93478261 0.95555556\n",
      " 0.97777778 0.93333333 0.95555556 0.95555556]\n"
     ]
    }
   ],
   "source": [
    "#Â Also it could have been written less verbosely like this:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator=pipe_lr,X=X_train,y=y_train,cv=10,n_jobs=1) # using the parameter `n_jobs` we can distribute the training across CPUS (-1 means use them all)\n",
    "print(f\"CV accuracy scores: {scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
